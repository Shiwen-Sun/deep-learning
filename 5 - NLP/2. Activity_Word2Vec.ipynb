{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activity : Word2Vec embeddings\n",
    "\n",
    "In the section about Text Classification, we used various approaches to embed the sentences to classify. Some of the reference techniques involved statistical modelling (e.g. BOW, TFIDF), and others involved learning representations for words.\n",
    "\n",
    "In this activity, we propose to implement Word2Vec embeddings in PyTorch (during the previous activity we used the gensim implementation of the algorithm)\n",
    "\n",
    "## 1. Corpus\n",
    "\n",
    "We will work on a very simple corpus :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    'louis est roi de france',\n",
    "    'marie est reine de france',\n",
    "    'louis est un homme',\n",
    "    'marie est une femme',\n",
    "    'paris est la capitale de la france',\n",
    "    'bruxelles est la capitale de la belgique',\n",
    "    'tokyo est la capitale du japon',\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the same implementation as what we did earlier, we propose to preprocess the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import FrenchStemmer\n",
    "from nltk import wordpunct_tokenize          \n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import words\n",
    "from string import punctuation\n",
    "import unidecode\n",
    "\n",
    "class FrenchTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.stopwords = set(stopwords.words('french'))\n",
    "        self.words = set(words.words())\n",
    "    def __call__(self, doc):\n",
    "        # tokenize words and punctuation\n",
    "        word_list = wordpunct_tokenize(doc)\n",
    "        # remove stopwords\n",
    "        word_list = [word for word in word_list if word not in self.stopwords]\n",
    "        # remove 1-character words\n",
    "        word_list = [word for word in word_list if len(word)>1]\n",
    "        # remove non alpha\n",
    "        word_list = [word for word in word_list if word.isalpha()]\n",
    "        return [unidecode.unidecode(t) for t in word_list]\n",
    "\n",
    "#Tokenizing sentences\n",
    "tok=FrenchTokenizer()\n",
    "text_for_word2vec=[tok(sent) for sent in corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary size :  13\n"
     ]
    }
   ],
   "source": [
    "#Vocabulary modelling : assigning indexes to words\n",
    "vocabulary = []\n",
    "for sentence in text_for_word2vec:\n",
    "    for token in sentence:\n",
    "        if token not in vocabulary:\n",
    "            vocabulary.append(token)\n",
    "\n",
    "word2idx = {w: idx for (idx, w) in enumerate(vocabulary)}\n",
    "idx2word = {idx: w for (idx, w) in enumerate(vocabulary)}\n",
    "\n",
    "vocabulary_size = len(vocabulary)\n",
    "print(\"vocabulary size : \", vocabulary_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Word2Vec architecture relies on predicting the tokens surrounding a given one (or vice versa). It is composed of a simple auto-encoder using one-hot encodings of the tokens as inputs and outputs, and a hidden layer of a chosen size that will represent the embedding.\n",
    "\n",
    "![skipgram](img/word2vecskipgram.png)\n",
    "\n",
    "Because input output vectors are one-hot encodings of the tokens, their dimensionnality is the size of the corpus (e.g : if there are 30 tokens in the vocabulary, the encoding will be of size 30, with each dimension representing the presence or absence of the token in the message)\n",
    "The hidden layer is of size N. Because of the auto-encoder structure, the hidden layer will be trained to constrain the information about tokens in a space of given dimensionnality.\n",
    "\n",
    "There are two approaches for Word2Vec :\n",
    "- the Continuous Bag-of-words (CBOW) appraoach, where the token is used to predict surrounding ones\n",
    "- the Skip-Gram approach, where context tokens are used to predict the central one\n",
    "\n",
    "In this exercice, we propose to implement the SkipGram approach. To train this model, we need token pairs (a context token and the central one).\n",
    "Token pairs are generated by going through all tokens and generating context tokens belonging to a window of a given size around the token."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class = \"alert alert-warning\">\n",
    "\n",
    "On génère des paires de mots sur lesquels on entraîne le réseau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "louis roi\n",
      "roi louis\n",
      "roi france\n",
      "france roi\n",
      "marie reine\n",
      "reine marie\n",
      "reine france\n",
      "france reine\n",
      "louis homme\n",
      "homme louis\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "window_size = 1\n",
    "\n",
    "idx_pairs = []\n",
    "# for each sentence\n",
    "for sentence in text_for_word2vec:\n",
    "    indices = [word2idx[word] for word in sentence]\n",
    "    # for each word, treated as center word\n",
    "    for center_word_pos in range(len(indices)):\n",
    "        # for each window position\n",
    "        for w in range(-window_size, window_size + 1):\n",
    "            context_word_pos = center_word_pos + w\n",
    "            # make soure not jump out sentence\n",
    "            if context_word_pos < 0 or context_word_pos >= len(indices) or center_word_pos == context_word_pos:\n",
    "                continue\n",
    "            context_word_idx = indices[context_word_pos]\n",
    "            idx_pairs.append((indices[center_word_pos], context_word_idx))\n",
    "\n",
    "idx_pairs = np.array(idx_pairs)\n",
    "\n",
    "#Printing first 10 pairs\n",
    "for i in range(10):\n",
    "    print(idx2word[idx_pairs[i][0]], idx2word[idx_pairs[i][1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use these pairs to train the network, we need to encode them in a layer :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def get_layer(word_idx):\n",
    "    x = torch.zeros(vocabulary_size).float()\n",
    "    x[word_idx] = 1.0\n",
    "    return x\n",
    "\n",
    "#exemple\n",
    "get_layer(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "Exercice : \n",
    "    \n",
    "Complete the class below to implement the architecture of the Skipgram.\n",
    "The class will include a method get_wv to get the word vector (i.e. the hidden layer) for a given word\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#COMPLETE HERE\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "# class skipgram(nn.Module):\n",
    "#     def __init__(self, vocab_size, embedding_dim):\n",
    "#         #vocab size : vocabulary size (corresponding to input and output dimensions)\n",
    "#         #embedding_dim : dimension of the embedding (hidden) layer\n",
    "#         super(skipgram, self).__init__()\n",
    "\n",
    "#     def forward(self, input):\n",
    "\n",
    "#         return output\n",
    "#     def get_wv(self,input):\n",
    "#         #get the word vector for a given input\n",
    "#         return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/1_skipgram.py\n",
    "class skipgram(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        #vocab size : vocabulary size (corresponding to input and output dimensions)\n",
    "        #embedding_dim : dimension of the embedding (hidden) layer\n",
    "        super(skipgram, self).__init__()\n",
    "        # pas de RNN ici, juste une réseau linéaire\n",
    "        self.lin1 = nn.Linear(vocab_size,embedding_dim) # une couche pour aller à l'état caché\n",
    "        self.lin2 = nn.Linear(embedding_dim,vocab_size) # une couche pour récupérer l'état de sortie\n",
    "        self.soft= nn.Softmax(dim=0) \n",
    "    def forward(self, input):\n",
    "        hidden = self.lin1(input)\n",
    "        output=self.lin2(hidden)\n",
    "        output=self.soft(output) # on fait passer l'état de sortie par le softmax\n",
    "        return output\n",
    "    def get_wv(self,input): # récupérer la représentation vectorielle d'un mot : on récupère l'activation de couche cachée\n",
    "        #get the word vector for a given input\n",
    "        return self.lin1(input).detach().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the network :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at epoch 0: 0.286283270145456\n",
      "Loss at epoch 10: 0.2713726833462715\n",
      "Loss at epoch 20: 0.26150408138831455\n",
      "Loss at epoch 30: 0.2539468618730704\n",
      "Loss at epoch 40: 0.24764183349907398\n",
      "Loss at epoch 50: 0.24146481789648533\n",
      "Loss at epoch 60: 0.23474724404513836\n",
      "Loss at epoch 70: 0.22746935797234377\n",
      "Loss at epoch 80: 0.22002030878017345\n",
      "Loss at epoch 90: 0.21287764764080444\n"
     ]
    }
   ],
   "source": [
    "model = skipgram(vocab_size=vocabulary_size,embedding_dim=2)\n",
    "loss_function = nn.BCELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "num_epochs = 100\n",
    "\n",
    "for epo in range(num_epochs):\n",
    "    loss_val = 0\n",
    "    for center_word, target_word in idx_pairs:\n",
    "        optimizer.zero_grad()\n",
    "        input = get_layer(center_word)\n",
    "        target = get_layer(target_word)\n",
    "        output=model(input)\n",
    "        loss = loss_function(output, target)\n",
    "        loss_val += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    if epo%10==0:\n",
    "        print(f'Loss at epoch {epo}: {loss_val/len(idx_pairs)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at the word embeddings. They could be used to represent individual tokens in a classification task, for instance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'louis': array([ 2.7596951e-04, -6.3711846e-01], dtype=float32), 'roi': array([ 0.31588852, -0.43377864], dtype=float32), 'france': array([0.23440444, 0.58936715], dtype=float32), 'marie': array([-0.6226934,  0.5823401], dtype=float32), 'reine': array([ 0.507831  , -0.48083264], dtype=float32), 'homme': array([0.66379166, 0.05007281], dtype=float32), 'femme': array([ 0.72154117, -0.02954477], dtype=float32), 'paris': array([1.0636444 , 0.58579195], dtype=float32), 'capitale': array([-0.3752228, -0.0287312], dtype=float32), 'bruxelles': array([0.99591804, 0.80952233], dtype=float32), 'belgique': array([1.1701809 , 0.53766656], dtype=float32), 'tokyo': array([1.1276237 , 0.62002265], dtype=float32), 'japon': array([1.2272844 , 0.46912107], dtype=float32)}\n"
     ]
    }
   ],
   "source": [
    "word_vectors = {w : model.get_wv(get_layer(word2idx[w])) for w in vocabulary}\n",
    "\n",
    "print(word_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class = \"alert alert-warning\">\n",
    "\n",
    "On a donc bien récupéré une représentation vectorielle des mots \n",
    "- Les mots sont représentés par des vecteurs (qu'on a forcé de taille 2 ici - on a pris une taille de couche cachée de 2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
